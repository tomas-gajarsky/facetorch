<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>facetorch.analyzer.detector.post API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>facetorch.analyzer.detector.post</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from abc import abstractmethod
from itertools import product as product
from math import ceil
from typing import List, Tuple, Union

import torch
from codetiming import Timer
from facetorch.base import BaseProcessor
from facetorch.datastruct import Detection, Dimensions, Face, ImageData, Location
from facetorch.logger import LoggerJsonFile
from facetorch.utils import rgb2bgr
from torchvision import transforms

logger = LoggerJsonFile().logger


class BaseDetPostProcessor(BaseProcessor):
    @Timer(
        &#34;BaseDetPostProcessor.__init__&#34;,
        &#34;{name}: {milliseconds:.2f} ms&#34;,
        logger=logger.debug,
    )
    def __init__(
        self,
        transform: transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;Base class for detector post processors.

        All detector post processors should subclass it.
        All subclass should overwrite:

        - Methods:``run``, used for running the processing

        Args:
            device (torch.device): Torch device cpu or cuda.
            transform (transforms.Compose): Transform compose object to be applied to the image.
            optimize_transform (bool): Whether to optimize the transform.

        &#34;&#34;&#34;
        super().__init__(transform, device, optimize_transform)

    @abstractmethod
    def run(
        self, data: ImageData, logits: Union[torch.Tensor, Tuple[torch.Tensor]]
    ) -&gt; ImageData:
        &#34;&#34;&#34;Abstract method that runs the detector post processing functionality
        and returns the data object.

        Args:
            data (ImageData): ImageData object containing the image tensor.
            logits (Union[torch.Tensor, Tuple[torch.Tensor]]): Output of the detector model.

        Returns:
            ImageData: Image data object with Detection tensors and detected Face objects.


        &#34;&#34;&#34;


class PriorBox:
    &#34;&#34;&#34;
    PriorBox class for generating prior boxes.

    Args:
        min_sizes (List[List[int]]): List of list of minimum sizes for each feature map.
        steps (List[int]): List of steps for each feature map.
        clip (bool): Whether to clip the prior boxes to the image boundaries.
    &#34;&#34;&#34;

    def __init__(self, min_sizes: List[List[int]], steps: List[int], clip: bool):
        self.min_sizes = [list(min_size) for min_size in min_sizes]
        self.steps = list(steps)
        self.clip = clip

    def forward(self, dims: Dimensions) -&gt; torch.Tensor:
        &#34;&#34;&#34;Generate prior boxes for each feature map.

        Args:
            dims (Dimensions): Dimensions of the image.

        Returns:
            torch.Tensor: Tensor of prior boxes.
        &#34;&#34;&#34;
        feature_maps = [
            [ceil(dims.height / step), ceil(dims.width / step)] for step in self.steps
        ]
        anchors = []
        for k, f in enumerate(feature_maps):
            min_sizes = self.min_sizes[k]
            for i, j in product(range(f[0]), range(f[1])):
                for min_size in min_sizes:
                    s_kx = min_size / dims.width
                    s_ky = min_size / dims.height
                    dense_cx = [x * self.steps[k] / dims.width for x in [j + 0.5]]
                    dense_cy = [y * self.steps[k] / dims.height for y in [i + 0.5]]
                    for cy, cx in product(dense_cy, dense_cx):
                        anchors.append([cx, cy, s_kx, s_ky])

        output = torch.Tensor(anchors)
        if self.clip:
            output.clamp_(min=0, max=1)
        return output


class PostRetFace(BaseDetPostProcessor):
    @Timer(&#34;PostRetFace.__init__&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def __init__(
        self,
        transform: transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
        confidence_threshold: float,
        top_k: int,
        nms_threshold: float,
        keep_top_k: int,
        score_threshold: float,
        prior_box: PriorBox,
        variance: List[float],
        reverse_colors: bool = False,
        expand_box_ratio: float = 0.0,
    ):
        &#34;&#34;&#34;Initialize the detector postprocessor. Modified from https://github.com/biubug6/Pytorch_Retinaface.

        Args:
            transform (Compose): Composed Torch transform object.
            device (torch.device): Torch device cpu or cuda.
            optimize_transform (bool): Whether to optimize the transform.
            confidence_threshold (float): Confidence threshold for face detection.
            top_k (int): Top K faces to keep before NMS.
            nms_threshold (float): NMS threshold.
            keep_top_k (int): Keep top K faces after NMS.
            score_threshold (float): Score threshold for face detection.
            prior_box (PriorBox): PriorBox object.
            variance (List[float]): Prior box variance.
            reverse_colors (bool): Whether to reverse the colors of the image tensor from RGB to BGR or vice versa. If False, the colors remain unchanged. Default: False.
            expand_box_ratio (float): Expand the box by this ratio. Default: 0.0.
        &#34;&#34;&#34;
        super().__init__(transform, device, optimize_transform)
        self.confidence_threshold = confidence_threshold
        self.top_k = top_k
        self.nms_threshold = nms_threshold
        self.keep_top_k = keep_top_k
        self.score_threshold = score_threshold
        self.prior_box = prior_box
        self.variance = list(variance)
        self.reverse_colors = reverse_colors
        self.expand_box_ratio = expand_box_ratio

    @Timer(&#34;PostRetFace.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(
        self,
        data: ImageData,
        logits: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
    ) -&gt; ImageData:
        &#34;&#34;&#34;Run the detector postprocessor.

        Args:
            data (ImageData): ImageData object containing the image tensor.
            logits (Union[torch.Tensor, Tuple[torch.Tensor]]): Output of the detector model.

        Returns:
            ImageData: Image data object with detection tensors and detected Face objects.
        &#34;&#34;&#34;
        data.det = Detection(loc=logits[0], conf=logits[1], landmarks=logits[2])

        if self.reverse_colors:
            data.tensor = rgb2bgr(data.tensor)

        data = self._process_dets(data)
        data = self._extract_faces(data)
        return data

    def _process_dets(self, data: ImageData) -&gt; ImageData:
        &#34;&#34;&#34;Compute the detections and add them to the data detector.

        Args:
            data (ImageData): Image data with with locations and confidences from detector.

        Returns:
            ImageData: Image data object with detections.
        &#34;&#34;&#34;

        def _decode(
            _loc: torch.Tensor, _priors: torch.Tensor, variances: List[float]
        ) -&gt; torch.Tensor:
            _boxes = torch.cat(
                (
                    _priors[:, :2] + _loc[:, :2] * variances[0] * _priors[:, 2:],
                    _priors[:, 2:] * torch.exp(_loc[:, 2:] * variances[1]),
                ),
                1,
            )
            _boxes[:, :2] -= _boxes[:, 2:] / 2
            _boxes[:, 2:] += _boxes[:, :2]
            return _boxes

        def _extract_boxes(_loc: torch.Tensor) -&gt; torch.Tensor:
            priors = self.prior_box.forward(data.dims)
            priors = priors.to(self.device)
            prior_data = priors.data
            _boxes = _decode(_loc.data.squeeze(0), prior_data, self.variance)
            img_scale = torch.Tensor([data.dims.width, data.dims.height]).repeat(2)
            _boxes = _boxes * img_scale.to(self.device)
            return _boxes

        def _nms(dets: torch.Tensor, thresh: float) -&gt; torch.Tensor:
            &#34;&#34;&#34;Non-maximum suppression.&#34;&#34;&#34;
            x1 = dets[:, 0]
            y1 = dets[:, 1]
            x2 = dets[:, 2]
            y2 = dets[:, 3]

            areas = (x2 - x1 + 1) * (y2 - y1 + 1)
            order = torch.arange(dets.shape[0], device=self.device)

            zero_tensor = torch.tensor(0.0).to(self.device)
            keep = []
            while order.size()[0] &gt; 0:
                i = order[0]
                keep.append(i)
                xx1 = torch.maximum(x1[i], x1[order[1:]])
                yy1 = torch.maximum(y1[i], y1[order[1:]])
                xx2 = torch.minimum(x2[i], x2[order[1:]])
                yy2 = torch.minimum(y2[i], y2[order[1:]])

                w = torch.maximum(zero_tensor, xx2 - xx1 + 1)
                h = torch.maximum(zero_tensor, yy2 - yy1 + 1)
                inter = torch.multiply(w, h)
                ovr = inter / (areas[i] + areas[order[1:]] - inter)

                inds = ovr &lt;= thresh
                order = order[1:][inds]

            if len(keep) &gt; 0:
                keep = torch.stack(keep)
            else:
                keep = torch.tensor([])

            return keep

        def _extract_dets(_conf: torch.Tensor, _boxes: torch.Tensor) -&gt; torch.Tensor:
            scores = _conf.squeeze(0).data[:, 1]
            # ignore low scores
            inds = scores &gt; self.confidence_threshold
            _boxes = _boxes[inds]
            scores = scores[inds]
            # keep top-K before NMS
            order = torch.argsort(scores, descending=True)[: self.top_k]
            _boxes = _boxes[order]
            scores = scores[order]
            # do NMS
            _dets = torch.hstack((_boxes, scores.unsqueeze(1)))
            keep = _nms(_dets, self.nms_threshold)

            if not keep.shape[0] == 0:
                _dets = _dets[keep, :]
                # keep top-K after NMS
                _dets = _dets[: self.keep_top_k, :]
                # keep dets with score &gt; score_threshold
                _dets = _dets[_dets[:, 4] &gt; self.score_threshold]

            return _dets

        data.det.boxes = _extract_boxes(data.det.loc)
        data.det.dets = _extract_dets(data.det.conf, data.det.boxes)
        return data

    def _extract_faces(self, data: ImageData) -&gt; ImageData:
        &#34;&#34;&#34;Extracts the faces from the original image using the detections.

        Args:
            data (ImageData): Image data with image tensor and detections.

        Returns:
            ImageData: Image data object with extracted faces.

        &#34;&#34;&#34;

        def _get_coordinates(_det: torch.Tensor) -&gt; Location:
            _det = torch.round(_det).int()
            loc = Location(
                x1=int(_det[0]),
                y1=int(_det[1]),
                x2=int(_det[2]),
                y2=int(_det[3]),
            )

            loc.expand(amount=self.expand_box_ratio)
            loc.form_square()

            return loc

        for indx, det in enumerate(data.det.dets):
            loc = _get_coordinates(det)
            face_tensor = data.tensor[0, :, loc.y1 : loc.y2, loc.x1 : loc.x2]
            dims = Dimensions(face_tensor.shape[-2], face_tensor.shape[-1])
            size_img = data.tensor.shape[-2] * data.tensor.shape[-1]
            size_ratio = (dims.height * dims.width) / size_img

            if not any([dim == 0 for dim in face_tensor.shape]):
                face = Face(
                    indx=indx, loc=loc, tensor=face_tensor, dims=dims, ratio=size_ratio
                )
                data.faces.append(face)

        return data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="facetorch.analyzer.detector.post.BaseDetPostProcessor"><code class="flex name class">
<span>class <span class="ident">BaseDetPostProcessor</span></span>
<span>(</span><span>transform:Â torchvision.transforms.transforms.Compose, device:Â torch.device, optimize_transform:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for detector post processors.</p>
<p>All detector post processors should subclass it.
All subclass should overwrite:</p>
<ul>
<li>Methods:<code>run</code>, used for running the processing</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>Torch device cpu or cuda.</dd>
<dt><strong><code>transform</code></strong> :&ensp;<code>transforms.Compose</code></dt>
<dd>Transform compose object to be applied to the image.</dd>
<dt><strong><code>optimize_transform</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to optimize the transform.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDetPostProcessor(BaseProcessor):
    @Timer(
        &#34;BaseDetPostProcessor.__init__&#34;,
        &#34;{name}: {milliseconds:.2f} ms&#34;,
        logger=logger.debug,
    )
    def __init__(
        self,
        transform: transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;Base class for detector post processors.

        All detector post processors should subclass it.
        All subclass should overwrite:

        - Methods:``run``, used for running the processing

        Args:
            device (torch.device): Torch device cpu or cuda.
            transform (transforms.Compose): Transform compose object to be applied to the image.
            optimize_transform (bool): Whether to optimize the transform.

        &#34;&#34;&#34;
        super().__init__(transform, device, optimize_transform)

    @abstractmethod
    def run(
        self, data: ImageData, logits: Union[torch.Tensor, Tuple[torch.Tensor]]
    ) -&gt; ImageData:
        &#34;&#34;&#34;Abstract method that runs the detector post processing functionality
        and returns the data object.

        Args:
            data (ImageData): ImageData object containing the image tensor.
            logits (Union[torch.Tensor, Tuple[torch.Tensor]]): Output of the detector model.

        Returns:
            ImageData: Image data object with Detection tensors and detected Face objects.


        &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="facetorch.base.BaseProcessor" href="../../base.html#facetorch.base.BaseProcessor">BaseProcessor</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="facetorch.analyzer.detector.post.PostRetFace" href="#facetorch.analyzer.detector.post.PostRetFace">PostRetFace</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="facetorch.analyzer.detector.post.BaseDetPostProcessor.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, data:Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a>, logits:Â Union[torch.Tensor,Â Tuple[torch.Tensor]]) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract method that runs the detector post processing functionality
and returns the data object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>ImageData</code></dt>
<dd>ImageData object containing the image tensor.</dd>
<dt><strong><code>logits</code></strong> :&ensp;<code>Union[torch.Tensor, Tuple[torch.Tensor]]</code></dt>
<dd>Output of the detector model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ImageData</code></dt>
<dd>Image data object with Detection tensors and detected Face objects.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def run(
    self, data: ImageData, logits: Union[torch.Tensor, Tuple[torch.Tensor]]
) -&gt; ImageData:
    &#34;&#34;&#34;Abstract method that runs the detector post processing functionality
    and returns the data object.

    Args:
        data (ImageData): ImageData object containing the image tensor.
        logits (Union[torch.Tensor, Tuple[torch.Tensor]]): Output of the detector model.

    Returns:
        ImageData: Image data object with Detection tensors and detected Face objects.


    &#34;&#34;&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="facetorch.base.BaseProcessor" href="../../base.html#facetorch.base.BaseProcessor">BaseProcessor</a></b></code>:
<ul class="hlist">
<li><code><a title="facetorch.base.BaseProcessor.optimize" href="../../base.html#facetorch.base.BaseProcessor.optimize">optimize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="facetorch.analyzer.detector.post.PriorBox"><code class="flex name class">
<span>class <span class="ident">PriorBox</span></span>
<span>(</span><span>min_sizes:Â List[List[int]], steps:Â List[int], clip:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>PriorBox class for generating prior boxes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>min_sizes</code></strong> :&ensp;<code>List[List[int]]</code></dt>
<dd>List of list of minimum sizes for each feature map.</dd>
<dt><strong><code>steps</code></strong> :&ensp;<code>List[int]</code></dt>
<dd>List of steps for each feature map.</dd>
<dt><strong><code>clip</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to clip the prior boxes to the image boundaries.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PriorBox:
    &#34;&#34;&#34;
    PriorBox class for generating prior boxes.

    Args:
        min_sizes (List[List[int]]): List of list of minimum sizes for each feature map.
        steps (List[int]): List of steps for each feature map.
        clip (bool): Whether to clip the prior boxes to the image boundaries.
    &#34;&#34;&#34;

    def __init__(self, min_sizes: List[List[int]], steps: List[int], clip: bool):
        self.min_sizes = [list(min_size) for min_size in min_sizes]
        self.steps = list(steps)
        self.clip = clip

    def forward(self, dims: Dimensions) -&gt; torch.Tensor:
        &#34;&#34;&#34;Generate prior boxes for each feature map.

        Args:
            dims (Dimensions): Dimensions of the image.

        Returns:
            torch.Tensor: Tensor of prior boxes.
        &#34;&#34;&#34;
        feature_maps = [
            [ceil(dims.height / step), ceil(dims.width / step)] for step in self.steps
        ]
        anchors = []
        for k, f in enumerate(feature_maps):
            min_sizes = self.min_sizes[k]
            for i, j in product(range(f[0]), range(f[1])):
                for min_size in min_sizes:
                    s_kx = min_size / dims.width
                    s_ky = min_size / dims.height
                    dense_cx = [x * self.steps[k] / dims.width for x in [j + 0.5]]
                    dense_cy = [y * self.steps[k] / dims.height for y in [i + 0.5]]
                    for cy, cx in product(dense_cy, dense_cx):
                        anchors.append([cx, cy, s_kx, s_ky])

        output = torch.Tensor(anchors)
        if self.clip:
            output.clamp_(min=0, max=1)
        return output</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="facetorch.analyzer.detector.post.PriorBox.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, dims:Â <a title="facetorch.datastruct.Dimensions" href="../../datastruct.html#facetorch.datastruct.Dimensions">Dimensions</a>) â€‘>Â torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Generate prior boxes for each feature map.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong> :&ensp;<code>Dimensions</code></dt>
<dd>Dimensions of the image.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Tensor of prior boxes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, dims: Dimensions) -&gt; torch.Tensor:
    &#34;&#34;&#34;Generate prior boxes for each feature map.

    Args:
        dims (Dimensions): Dimensions of the image.

    Returns:
        torch.Tensor: Tensor of prior boxes.
    &#34;&#34;&#34;
    feature_maps = [
        [ceil(dims.height / step), ceil(dims.width / step)] for step in self.steps
    ]
    anchors = []
    for k, f in enumerate(feature_maps):
        min_sizes = self.min_sizes[k]
        for i, j in product(range(f[0]), range(f[1])):
            for min_size in min_sizes:
                s_kx = min_size / dims.width
                s_ky = min_size / dims.height
                dense_cx = [x * self.steps[k] / dims.width for x in [j + 0.5]]
                dense_cy = [y * self.steps[k] / dims.height for y in [i + 0.5]]
                for cy, cx in product(dense_cy, dense_cx):
                    anchors.append([cx, cy, s_kx, s_ky])

    output = torch.Tensor(anchors)
    if self.clip:
        output.clamp_(min=0, max=1)
    return output</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="facetorch.analyzer.detector.post.PostRetFace"><code class="flex name class">
<span>class <span class="ident">PostRetFace</span></span>
<span>(</span><span>transform:Â torchvision.transforms.transforms.Compose, device:Â torch.device, optimize_transform:Â bool, confidence_threshold:Â float, top_k:Â int, nms_threshold:Â float, keep_top_k:Â int, score_threshold:Â float, prior_box:Â <a title="facetorch.analyzer.detector.post.PriorBox" href="#facetorch.analyzer.detector.post.PriorBox">PriorBox</a>, variance:Â List[float], reverse_colors:Â boolÂ =Â False, expand_box_ratio:Â floatÂ =Â 0.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the detector postprocessor. Modified from <a href="https://github.com/biubug6/Pytorch_Retinaface.">https://github.com/biubug6/Pytorch_Retinaface.</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transform</code></strong> :&ensp;<code>Compose</code></dt>
<dd>Composed Torch transform object.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>Torch device cpu or cuda.</dd>
<dt><strong><code>optimize_transform</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to optimize the transform.</dd>
<dt><strong><code>confidence_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Confidence threshold for face detection.</dd>
<dt><strong><code>top_k</code></strong> :&ensp;<code>int</code></dt>
<dd>Top K faces to keep before NMS.</dd>
<dt><strong><code>nms_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>NMS threshold.</dd>
<dt><strong><code>keep_top_k</code></strong> :&ensp;<code>int</code></dt>
<dd>Keep top K faces after NMS.</dd>
<dt><strong><code>score_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Score threshold for face detection.</dd>
<dt><strong><code>prior_box</code></strong> :&ensp;<code><a title="facetorch.analyzer.detector.post.PriorBox" href="#facetorch.analyzer.detector.post.PriorBox">PriorBox</a></code></dt>
<dd>PriorBox object.</dd>
<dt><strong><code>variance</code></strong> :&ensp;<code>List[float]</code></dt>
<dd>Prior box variance.</dd>
<dt><strong><code>reverse_colors</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to reverse the colors of the image tensor from RGB to BGR or vice versa. If False, the colors remain unchanged. Default: False.</dd>
<dt><strong><code>expand_box_ratio</code></strong> :&ensp;<code>float</code></dt>
<dd>Expand the box by this ratio. Default: 0.0.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PostRetFace(BaseDetPostProcessor):
    @Timer(&#34;PostRetFace.__init__&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def __init__(
        self,
        transform: transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
        confidence_threshold: float,
        top_k: int,
        nms_threshold: float,
        keep_top_k: int,
        score_threshold: float,
        prior_box: PriorBox,
        variance: List[float],
        reverse_colors: bool = False,
        expand_box_ratio: float = 0.0,
    ):
        &#34;&#34;&#34;Initialize the detector postprocessor. Modified from https://github.com/biubug6/Pytorch_Retinaface.

        Args:
            transform (Compose): Composed Torch transform object.
            device (torch.device): Torch device cpu or cuda.
            optimize_transform (bool): Whether to optimize the transform.
            confidence_threshold (float): Confidence threshold for face detection.
            top_k (int): Top K faces to keep before NMS.
            nms_threshold (float): NMS threshold.
            keep_top_k (int): Keep top K faces after NMS.
            score_threshold (float): Score threshold for face detection.
            prior_box (PriorBox): PriorBox object.
            variance (List[float]): Prior box variance.
            reverse_colors (bool): Whether to reverse the colors of the image tensor from RGB to BGR or vice versa. If False, the colors remain unchanged. Default: False.
            expand_box_ratio (float): Expand the box by this ratio. Default: 0.0.
        &#34;&#34;&#34;
        super().__init__(transform, device, optimize_transform)
        self.confidence_threshold = confidence_threshold
        self.top_k = top_k
        self.nms_threshold = nms_threshold
        self.keep_top_k = keep_top_k
        self.score_threshold = score_threshold
        self.prior_box = prior_box
        self.variance = list(variance)
        self.reverse_colors = reverse_colors
        self.expand_box_ratio = expand_box_ratio

    @Timer(&#34;PostRetFace.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(
        self,
        data: ImageData,
        logits: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
    ) -&gt; ImageData:
        &#34;&#34;&#34;Run the detector postprocessor.

        Args:
            data (ImageData): ImageData object containing the image tensor.
            logits (Union[torch.Tensor, Tuple[torch.Tensor]]): Output of the detector model.

        Returns:
            ImageData: Image data object with detection tensors and detected Face objects.
        &#34;&#34;&#34;
        data.det = Detection(loc=logits[0], conf=logits[1], landmarks=logits[2])

        if self.reverse_colors:
            data.tensor = rgb2bgr(data.tensor)

        data = self._process_dets(data)
        data = self._extract_faces(data)
        return data

    def _process_dets(self, data: ImageData) -&gt; ImageData:
        &#34;&#34;&#34;Compute the detections and add them to the data detector.

        Args:
            data (ImageData): Image data with with locations and confidences from detector.

        Returns:
            ImageData: Image data object with detections.
        &#34;&#34;&#34;

        def _decode(
            _loc: torch.Tensor, _priors: torch.Tensor, variances: List[float]
        ) -&gt; torch.Tensor:
            _boxes = torch.cat(
                (
                    _priors[:, :2] + _loc[:, :2] * variances[0] * _priors[:, 2:],
                    _priors[:, 2:] * torch.exp(_loc[:, 2:] * variances[1]),
                ),
                1,
            )
            _boxes[:, :2] -= _boxes[:, 2:] / 2
            _boxes[:, 2:] += _boxes[:, :2]
            return _boxes

        def _extract_boxes(_loc: torch.Tensor) -&gt; torch.Tensor:
            priors = self.prior_box.forward(data.dims)
            priors = priors.to(self.device)
            prior_data = priors.data
            _boxes = _decode(_loc.data.squeeze(0), prior_data, self.variance)
            img_scale = torch.Tensor([data.dims.width, data.dims.height]).repeat(2)
            _boxes = _boxes * img_scale.to(self.device)
            return _boxes

        def _nms(dets: torch.Tensor, thresh: float) -&gt; torch.Tensor:
            &#34;&#34;&#34;Non-maximum suppression.&#34;&#34;&#34;
            x1 = dets[:, 0]
            y1 = dets[:, 1]
            x2 = dets[:, 2]
            y2 = dets[:, 3]

            areas = (x2 - x1 + 1) * (y2 - y1 + 1)
            order = torch.arange(dets.shape[0], device=self.device)

            zero_tensor = torch.tensor(0.0).to(self.device)
            keep = []
            while order.size()[0] &gt; 0:
                i = order[0]
                keep.append(i)
                xx1 = torch.maximum(x1[i], x1[order[1:]])
                yy1 = torch.maximum(y1[i], y1[order[1:]])
                xx2 = torch.minimum(x2[i], x2[order[1:]])
                yy2 = torch.minimum(y2[i], y2[order[1:]])

                w = torch.maximum(zero_tensor, xx2 - xx1 + 1)
                h = torch.maximum(zero_tensor, yy2 - yy1 + 1)
                inter = torch.multiply(w, h)
                ovr = inter / (areas[i] + areas[order[1:]] - inter)

                inds = ovr &lt;= thresh
                order = order[1:][inds]

            if len(keep) &gt; 0:
                keep = torch.stack(keep)
            else:
                keep = torch.tensor([])

            return keep

        def _extract_dets(_conf: torch.Tensor, _boxes: torch.Tensor) -&gt; torch.Tensor:
            scores = _conf.squeeze(0).data[:, 1]
            # ignore low scores
            inds = scores &gt; self.confidence_threshold
            _boxes = _boxes[inds]
            scores = scores[inds]
            # keep top-K before NMS
            order = torch.argsort(scores, descending=True)[: self.top_k]
            _boxes = _boxes[order]
            scores = scores[order]
            # do NMS
            _dets = torch.hstack((_boxes, scores.unsqueeze(1)))
            keep = _nms(_dets, self.nms_threshold)

            if not keep.shape[0] == 0:
                _dets = _dets[keep, :]
                # keep top-K after NMS
                _dets = _dets[: self.keep_top_k, :]
                # keep dets with score &gt; score_threshold
                _dets = _dets[_dets[:, 4] &gt; self.score_threshold]

            return _dets

        data.det.boxes = _extract_boxes(data.det.loc)
        data.det.dets = _extract_dets(data.det.conf, data.det.boxes)
        return data

    def _extract_faces(self, data: ImageData) -&gt; ImageData:
        &#34;&#34;&#34;Extracts the faces from the original image using the detections.

        Args:
            data (ImageData): Image data with image tensor and detections.

        Returns:
            ImageData: Image data object with extracted faces.

        &#34;&#34;&#34;

        def _get_coordinates(_det: torch.Tensor) -&gt; Location:
            _det = torch.round(_det).int()
            loc = Location(
                x1=int(_det[0]),
                y1=int(_det[1]),
                x2=int(_det[2]),
                y2=int(_det[3]),
            )

            loc.expand(amount=self.expand_box_ratio)
            loc.form_square()

            return loc

        for indx, det in enumerate(data.det.dets):
            loc = _get_coordinates(det)
            face_tensor = data.tensor[0, :, loc.y1 : loc.y2, loc.x1 : loc.x2]
            dims = Dimensions(face_tensor.shape[-2], face_tensor.shape[-1])
            size_img = data.tensor.shape[-2] * data.tensor.shape[-1]
            size_ratio = (dims.height * dims.width) / size_img

            if not any([dim == 0 for dim in face_tensor.shape]):
                face = Face(
                    indx=indx, loc=loc, tensor=face_tensor, dims=dims, ratio=size_ratio
                )
                data.faces.append(face)

        return data</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="facetorch.analyzer.detector.post.BaseDetPostProcessor" href="#facetorch.analyzer.detector.post.BaseDetPostProcessor">BaseDetPostProcessor</a></li>
<li><a title="facetorch.base.BaseProcessor" href="../../base.html#facetorch.base.BaseProcessor">BaseProcessor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="facetorch.analyzer.detector.post.PostRetFace.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, data:Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a>, logits:Â Union[torch.Tensor,Â Tuple[torch.Tensor,Â torch.Tensor,Â torch.Tensor]]) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"><p>Run the detector postprocessor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>ImageData</code></dt>
<dd>ImageData object containing the image tensor.</dd>
<dt><strong><code>logits</code></strong> :&ensp;<code>Union[torch.Tensor, Tuple[torch.Tensor]]</code></dt>
<dd>Output of the detector model.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ImageData</code></dt>
<dd>Image data object with detection tensors and detected Face objects.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Timer(&#34;PostRetFace.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
def run(
    self,
    data: ImageData,
    logits: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
) -&gt; ImageData:
    &#34;&#34;&#34;Run the detector postprocessor.

    Args:
        data (ImageData): ImageData object containing the image tensor.
        logits (Union[torch.Tensor, Tuple[torch.Tensor]]): Output of the detector model.

    Returns:
        ImageData: Image data object with detection tensors and detected Face objects.
    &#34;&#34;&#34;
    data.det = Detection(loc=logits[0], conf=logits[1], landmarks=logits[2])

    if self.reverse_colors:
        data.tensor = rgb2bgr(data.tensor)

    data = self._process_dets(data)
    data = self._extract_faces(data)
    return data</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="facetorch.analyzer.detector.post.BaseDetPostProcessor" href="#facetorch.analyzer.detector.post.BaseDetPostProcessor">BaseDetPostProcessor</a></b></code>:
<ul class="hlist">
<li><code><a title="facetorch.analyzer.detector.post.BaseDetPostProcessor.optimize" href="../../base.html#facetorch.base.BaseProcessor.optimize">optimize</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="facetorch.analyzer.detector" href="index.html">facetorch.analyzer.detector</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="facetorch.analyzer.detector.post.BaseDetPostProcessor" href="#facetorch.analyzer.detector.post.BaseDetPostProcessor">BaseDetPostProcessor</a></code></h4>
<ul class="">
<li><code><a title="facetorch.analyzer.detector.post.BaseDetPostProcessor.run" href="#facetorch.analyzer.detector.post.BaseDetPostProcessor.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="facetorch.analyzer.detector.post.PriorBox" href="#facetorch.analyzer.detector.post.PriorBox">PriorBox</a></code></h4>
<ul class="">
<li><code><a title="facetorch.analyzer.detector.post.PriorBox.forward" href="#facetorch.analyzer.detector.post.PriorBox.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="facetorch.analyzer.detector.post.PostRetFace" href="#facetorch.analyzer.detector.post.PostRetFace">PostRetFace</a></code></h4>
<ul class="">
<li><code><a title="facetorch.analyzer.detector.post.PostRetFace.run" href="#facetorch.analyzer.detector.post.PostRetFace.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>