<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>facetorch.analyzer.reader.core API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>facetorch.analyzer.reader.core</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import io
import requests
from PIL import Image
import numpy as np
import torch
import torchvision
from codetiming import Timer
from typing import Union
from facetorch.base import BaseReader
from facetorch.datastruct import ImageData
from facetorch.logger import LoggerJsonFile

logger = LoggerJsonFile().logger


class UniversalReader(BaseReader):
    def __init__(
        self,
        transform: torchvision.transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;UniversalReader can read images from a path, URL, tensor, numpy array, bytes or PIL Image and return an ImageData object containing the image tensor.

        Args:
            transform (torchvision.transforms.Compose): Transform compose object to be applied to the image, if fix_image_size is True.
            device (torch.device): Torch device cpu or cuda object.
            optimize_transform (bool): Whether to optimize the transforms that are: resizing the image to a fixed size.

        &#34;&#34;&#34;
        super().__init__(transform, device, optimize_transform)

    @Timer(&#34;UniversalReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(
        self,
        image_source: Union[str, torch.Tensor, np.ndarray, bytes, Image.Image],
        fix_img_size: bool = False,
    ) -&gt; ImageData:
        &#34;&#34;&#34;Reads an image from a path, URL, tensor, numpy array, bytes or PIL Image and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

        Args:
            image_source (Union[str, torch.Tensor, np.ndarray, bytes, Image.Image]): Image source to be read.
            fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

        Returns:
            ImageData: ImageData object with image tensor and pil Image.
        &#34;&#34;&#34;
        if isinstance(image_source, str):
            if image_source.startswith(&#34;http&#34;):
                return self.read_image_from_url(image_source, fix_img_size)
            else:
                return self.read_image_from_path(image_source, fix_img_size)
        elif isinstance(image_source, torch.Tensor):
            return self.read_tensor(image_source, fix_img_size)
        elif isinstance(image_source, np.ndarray):
            return self.read_numpy_array(image_source, fix_img_size)
        elif isinstance(image_source, bytes):
            return self.read_image_from_bytes(image_source, fix_img_size)
        elif isinstance(image_source, Image.Image):
            return self.read_pil_image(image_source, fix_img_size)
        else:
            raise ValueError(&#34;Unsupported data type&#34;)

    def read_tensor(self, tensor: torch.Tensor, fix_img_size: bool) -&gt; ImageData:
        return self.process_tensor(tensor, fix_img_size)

    def read_pil_image(self, pil_image: Image.Image, fix_img_size: bool) -&gt; ImageData:
        tensor = torchvision.transforms.functional.to_tensor(pil_image)
        return self.process_tensor(tensor, fix_img_size)

    def read_numpy_array(self, array: np.ndarray, fix_img_size: bool) -&gt; ImageData:
        pil_image = Image.fromarray(array, mode=&#34;RGB&#34;)
        return self.read_pil_image(pil_image, fix_img_size)

    def read_image_from_bytes(
        self, image_bytes: bytes, fix_img_size: bool
    ) -&gt; ImageData:
        pil_image = Image.open(io.BytesIO(image_bytes))
        return self.read_pil_image(pil_image, fix_img_size)

    def read_image_from_path(self, path_image: str, fix_img_size: bool) -&gt; ImageData:
        try:
            image_tensor = torchvision.io.read_image(path_image)
        except Exception as e:
            logger.error(f&#34;Failed to read image from path {path_image}: {e}&#34;)
            raise ValueError(f&#34;Could not read image from path {path_image}: {e}&#34;) from e

        return self.process_tensor(image_tensor, fix_img_size)

    def read_image_from_url(self, url: str, fix_img_size: bool) -&gt; ImageData:
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            logger.error(f&#34;Failed to fetch image from URL {url}: {e}&#34;)
            raise ValueError(f&#34;Could not fetch image from URL {url}: {e}&#34;) from e

        image_bytes = response.content
        return self.read_image_from_bytes(image_bytes, fix_img_size)


class ImageReader(BaseReader):
    def __init__(
        self,
        transform: torchvision.transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;ImageReader is a wrapper around a functionality for reading images by Torchvision.

        Args:
            transform (torchvision.transforms.Compose): Transform compose object to be applied to the image, if fix_image_size is True.
            device (torch.device): Torch device cpu or cuda object.
            optimize_transform (bool): Whether to optimize the transforms that are: resizing the image to a fixed size.

        &#34;&#34;&#34;
        super().__init__(
            transform,
            device,
            optimize_transform,
        )

    @Timer(&#34;ImageReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(self, path_image: str, fix_img_size: bool = False) -&gt; ImageData:
        &#34;&#34;&#34;Reads an image from a path and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

        Args:
            path_image (str): Path to the image.
            fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

        Returns:
            ImageData: ImageData object with image tensor and pil Image.
        &#34;&#34;&#34;
        data = ImageData(path_input=path_image)
        data.img = torchvision.io.read_image(
            data.path_input, mode=torchvision.io.ImageReadMode.RGB
        )
        data.img = data.img.unsqueeze(0)
        data.img = data.img.to(self.device)

        if fix_img_size:
            data.img = self.transform(data.img)

        data.tensor = data.img.type(torch.float32)
        data.img = data.img.squeeze(0).cpu()
        data.set_dims()

        return data


class TensorReader(BaseReader):
    def __init__(
        self,
        transform: torchvision.transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;TensorReader is a wrapper around a functionality for reading tensors by Torchvision.

        Args:
            transform (torchvision.transforms.Compose): Transform compose object to be applied to the image, if fix_image_size is True.
            device (torch.device): Torch device cpu or cuda object.
            optimize_transform (bool): Whether to optimize the transforms that are: resizing the image to a fixed size.

        &#34;&#34;&#34;
        super().__init__(
            transform,
            device,
            optimize_transform,
        )

    @Timer(&#34;TensorReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(self, tensor: torch.Tensor, fix_img_size: bool = False) -&gt; ImageData:
        &#34;&#34;&#34;Reads a tensor and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

        Args:
            tensor (torch.Tensor): Tensor of a single image with RGB values between 0-255 and shape (channels, height, width).
            fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

        Returns:
            ImageData: ImageData object with image tensor and pil Image.
        &#34;&#34;&#34;
        return self.process_tensor(tensor, fix_img_size)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="facetorch.analyzer.reader.core.UniversalReader"><code class="flex name class">
<span>class <span class="ident">UniversalReader</span></span>
<span>(</span><span>transform:Â torchvision.transforms.transforms.Compose, device:Â torch.device, optimize_transform:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>UniversalReader can read images from a path, URL, tensor, numpy array, bytes or PIL Image and return an ImageData object containing the image tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transform</code></strong> :&ensp;<code>torchvision.transforms.Compose</code></dt>
<dd>Transform compose object to be applied to the image, if fix_image_size is True.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>Torch device cpu or cuda object.</dd>
<dt><strong><code>optimize_transform</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to optimize the transforms that are: resizing the image to a fixed size.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UniversalReader(BaseReader):
    def __init__(
        self,
        transform: torchvision.transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;UniversalReader can read images from a path, URL, tensor, numpy array, bytes or PIL Image and return an ImageData object containing the image tensor.

        Args:
            transform (torchvision.transforms.Compose): Transform compose object to be applied to the image, if fix_image_size is True.
            device (torch.device): Torch device cpu or cuda object.
            optimize_transform (bool): Whether to optimize the transforms that are: resizing the image to a fixed size.

        &#34;&#34;&#34;
        super().__init__(transform, device, optimize_transform)

    @Timer(&#34;UniversalReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(
        self,
        image_source: Union[str, torch.Tensor, np.ndarray, bytes, Image.Image],
        fix_img_size: bool = False,
    ) -&gt; ImageData:
        &#34;&#34;&#34;Reads an image from a path, URL, tensor, numpy array, bytes or PIL Image and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

        Args:
            image_source (Union[str, torch.Tensor, np.ndarray, bytes, Image.Image]): Image source to be read.
            fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

        Returns:
            ImageData: ImageData object with image tensor and pil Image.
        &#34;&#34;&#34;
        if isinstance(image_source, str):
            if image_source.startswith(&#34;http&#34;):
                return self.read_image_from_url(image_source, fix_img_size)
            else:
                return self.read_image_from_path(image_source, fix_img_size)
        elif isinstance(image_source, torch.Tensor):
            return self.read_tensor(image_source, fix_img_size)
        elif isinstance(image_source, np.ndarray):
            return self.read_numpy_array(image_source, fix_img_size)
        elif isinstance(image_source, bytes):
            return self.read_image_from_bytes(image_source, fix_img_size)
        elif isinstance(image_source, Image.Image):
            return self.read_pil_image(image_source, fix_img_size)
        else:
            raise ValueError(&#34;Unsupported data type&#34;)

    def read_tensor(self, tensor: torch.Tensor, fix_img_size: bool) -&gt; ImageData:
        return self.process_tensor(tensor, fix_img_size)

    def read_pil_image(self, pil_image: Image.Image, fix_img_size: bool) -&gt; ImageData:
        tensor = torchvision.transforms.functional.to_tensor(pil_image)
        return self.process_tensor(tensor, fix_img_size)

    def read_numpy_array(self, array: np.ndarray, fix_img_size: bool) -&gt; ImageData:
        pil_image = Image.fromarray(array, mode=&#34;RGB&#34;)
        return self.read_pil_image(pil_image, fix_img_size)

    def read_image_from_bytes(
        self, image_bytes: bytes, fix_img_size: bool
    ) -&gt; ImageData:
        pil_image = Image.open(io.BytesIO(image_bytes))
        return self.read_pil_image(pil_image, fix_img_size)

    def read_image_from_path(self, path_image: str, fix_img_size: bool) -&gt; ImageData:
        try:
            image_tensor = torchvision.io.read_image(path_image)
        except Exception as e:
            logger.error(f&#34;Failed to read image from path {path_image}: {e}&#34;)
            raise ValueError(f&#34;Could not read image from path {path_image}: {e}&#34;) from e

        return self.process_tensor(image_tensor, fix_img_size)

    def read_image_from_url(self, url: str, fix_img_size: bool) -&gt; ImageData:
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            logger.error(f&#34;Failed to fetch image from URL {url}: {e}&#34;)
            raise ValueError(f&#34;Could not fetch image from URL {url}: {e}&#34;) from e

        image_bytes = response.content
        return self.read_image_from_bytes(image_bytes, fix_img_size)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="facetorch.base.BaseReader" href="../../base.html#facetorch.base.BaseReader">BaseReader</a></li>
<li><a title="facetorch.base.BaseProcessor" href="../../base.html#facetorch.base.BaseProcessor">BaseProcessor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="facetorch.analyzer.reader.core.UniversalReader.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, image_source:Â Union[str,Â torch.Tensor,Â numpy.ndarray,Â bytes,Â PIL.Image.Image], fix_img_size:Â boolÂ =Â False) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"><p>Reads an image from a path, URL, tensor, numpy array, bytes or PIL Image and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image_source</code></strong> :&ensp;<code>Union[str, torch.Tensor, np.ndarray, bytes, Image.Image]</code></dt>
<dd>Image source to be read.</dd>
<dt><strong><code>fix_img_size</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ImageData</code></dt>
<dd>ImageData object with image tensor and pil Image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Timer(&#34;UniversalReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
def run(
    self,
    image_source: Union[str, torch.Tensor, np.ndarray, bytes, Image.Image],
    fix_img_size: bool = False,
) -&gt; ImageData:
    &#34;&#34;&#34;Reads an image from a path, URL, tensor, numpy array, bytes or PIL Image and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

    Args:
        image_source (Union[str, torch.Tensor, np.ndarray, bytes, Image.Image]): Image source to be read.
        fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

    Returns:
        ImageData: ImageData object with image tensor and pil Image.
    &#34;&#34;&#34;
    if isinstance(image_source, str):
        if image_source.startswith(&#34;http&#34;):
            return self.read_image_from_url(image_source, fix_img_size)
        else:
            return self.read_image_from_path(image_source, fix_img_size)
    elif isinstance(image_source, torch.Tensor):
        return self.read_tensor(image_source, fix_img_size)
    elif isinstance(image_source, np.ndarray):
        return self.read_numpy_array(image_source, fix_img_size)
    elif isinstance(image_source, bytes):
        return self.read_image_from_bytes(image_source, fix_img_size)
    elif isinstance(image_source, Image.Image):
        return self.read_pil_image(image_source, fix_img_size)
    else:
        raise ValueError(&#34;Unsupported data type&#34;)</code></pre>
</details>
</dd>
<dt id="facetorch.analyzer.reader.core.UniversalReader.read_tensor"><code class="name flex">
<span>def <span class="ident">read_tensor</span></span>(<span>self, tensor:Â torch.Tensor, fix_img_size:Â bool) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_tensor(self, tensor: torch.Tensor, fix_img_size: bool) -&gt; ImageData:
    return self.process_tensor(tensor, fix_img_size)</code></pre>
</details>
</dd>
<dt id="facetorch.analyzer.reader.core.UniversalReader.read_pil_image"><code class="name flex">
<span>def <span class="ident">read_pil_image</span></span>(<span>self, pil_image:Â PIL.Image.Image, fix_img_size:Â bool) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_pil_image(self, pil_image: Image.Image, fix_img_size: bool) -&gt; ImageData:
    tensor = torchvision.transforms.functional.to_tensor(pil_image)
    return self.process_tensor(tensor, fix_img_size)</code></pre>
</details>
</dd>
<dt id="facetorch.analyzer.reader.core.UniversalReader.read_numpy_array"><code class="name flex">
<span>def <span class="ident">read_numpy_array</span></span>(<span>self, array:Â numpy.ndarray, fix_img_size:Â bool) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_numpy_array(self, array: np.ndarray, fix_img_size: bool) -&gt; ImageData:
    pil_image = Image.fromarray(array, mode=&#34;RGB&#34;)
    return self.read_pil_image(pil_image, fix_img_size)</code></pre>
</details>
</dd>
<dt id="facetorch.analyzer.reader.core.UniversalReader.read_image_from_bytes"><code class="name flex">
<span>def <span class="ident">read_image_from_bytes</span></span>(<span>self, image_bytes:Â bytes, fix_img_size:Â bool) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_image_from_bytes(
    self, image_bytes: bytes, fix_img_size: bool
) -&gt; ImageData:
    pil_image = Image.open(io.BytesIO(image_bytes))
    return self.read_pil_image(pil_image, fix_img_size)</code></pre>
</details>
</dd>
<dt id="facetorch.analyzer.reader.core.UniversalReader.read_image_from_path"><code class="name flex">
<span>def <span class="ident">read_image_from_path</span></span>(<span>self, path_image:Â str, fix_img_size:Â bool) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_image_from_path(self, path_image: str, fix_img_size: bool) -&gt; ImageData:
    try:
        image_tensor = torchvision.io.read_image(path_image)
    except Exception as e:
        logger.error(f&#34;Failed to read image from path {path_image}: {e}&#34;)
        raise ValueError(f&#34;Could not read image from path {path_image}: {e}&#34;) from e

    return self.process_tensor(image_tensor, fix_img_size)</code></pre>
</details>
</dd>
<dt id="facetorch.analyzer.reader.core.UniversalReader.read_image_from_url"><code class="name flex">
<span>def <span class="ident">read_image_from_url</span></span>(<span>self, url:Â str, fix_img_size:Â bool) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_image_from_url(self, url: str, fix_img_size: bool) -&gt; ImageData:
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f&#34;Failed to fetch image from URL {url}: {e}&#34;)
        raise ValueError(f&#34;Could not fetch image from URL {url}: {e}&#34;) from e

    image_bytes = response.content
    return self.read_image_from_bytes(image_bytes, fix_img_size)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="facetorch.base.BaseReader" href="../../base.html#facetorch.base.BaseReader">BaseReader</a></b></code>:
<ul class="hlist">
<li><code><a title="facetorch.base.BaseReader.optimize" href="../../base.html#facetorch.base.BaseProcessor.optimize">optimize</a></code></li>
<li><code><a title="facetorch.base.BaseReader.process_tensor" href="../../base.html#facetorch.base.BaseReader.process_tensor">process_tensor</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="facetorch.analyzer.reader.core.ImageReader"><code class="flex name class">
<span>class <span class="ident">ImageReader</span></span>
<span>(</span><span>transform:Â torchvision.transforms.transforms.Compose, device:Â torch.device, optimize_transform:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>ImageReader is a wrapper around a functionality for reading images by Torchvision.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transform</code></strong> :&ensp;<code>torchvision.transforms.Compose</code></dt>
<dd>Transform compose object to be applied to the image, if fix_image_size is True.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>Torch device cpu or cuda object.</dd>
<dt><strong><code>optimize_transform</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to optimize the transforms that are: resizing the image to a fixed size.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ImageReader(BaseReader):
    def __init__(
        self,
        transform: torchvision.transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;ImageReader is a wrapper around a functionality for reading images by Torchvision.

        Args:
            transform (torchvision.transforms.Compose): Transform compose object to be applied to the image, if fix_image_size is True.
            device (torch.device): Torch device cpu or cuda object.
            optimize_transform (bool): Whether to optimize the transforms that are: resizing the image to a fixed size.

        &#34;&#34;&#34;
        super().__init__(
            transform,
            device,
            optimize_transform,
        )

    @Timer(&#34;ImageReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(self, path_image: str, fix_img_size: bool = False) -&gt; ImageData:
        &#34;&#34;&#34;Reads an image from a path and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

        Args:
            path_image (str): Path to the image.
            fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

        Returns:
            ImageData: ImageData object with image tensor and pil Image.
        &#34;&#34;&#34;
        data = ImageData(path_input=path_image)
        data.img = torchvision.io.read_image(
            data.path_input, mode=torchvision.io.ImageReadMode.RGB
        )
        data.img = data.img.unsqueeze(0)
        data.img = data.img.to(self.device)

        if fix_img_size:
            data.img = self.transform(data.img)

        data.tensor = data.img.type(torch.float32)
        data.img = data.img.squeeze(0).cpu()
        data.set_dims()

        return data</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="facetorch.base.BaseReader" href="../../base.html#facetorch.base.BaseReader">BaseReader</a></li>
<li><a title="facetorch.base.BaseProcessor" href="../../base.html#facetorch.base.BaseProcessor">BaseProcessor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="facetorch.analyzer.reader.core.ImageReader.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, path_image:Â str, fix_img_size:Â boolÂ =Â False) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"><p>Reads an image from a path and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path_image</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the image.</dd>
<dt><strong><code>fix_img_size</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ImageData</code></dt>
<dd>ImageData object with image tensor and pil Image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Timer(&#34;ImageReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
def run(self, path_image: str, fix_img_size: bool = False) -&gt; ImageData:
    &#34;&#34;&#34;Reads an image from a path and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

    Args:
        path_image (str): Path to the image.
        fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

    Returns:
        ImageData: ImageData object with image tensor and pil Image.
    &#34;&#34;&#34;
    data = ImageData(path_input=path_image)
    data.img = torchvision.io.read_image(
        data.path_input, mode=torchvision.io.ImageReadMode.RGB
    )
    data.img = data.img.unsqueeze(0)
    data.img = data.img.to(self.device)

    if fix_img_size:
        data.img = self.transform(data.img)

    data.tensor = data.img.type(torch.float32)
    data.img = data.img.squeeze(0).cpu()
    data.set_dims()

    return data</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="facetorch.base.BaseReader" href="../../base.html#facetorch.base.BaseReader">BaseReader</a></b></code>:
<ul class="hlist">
<li><code><a title="facetorch.base.BaseReader.optimize" href="../../base.html#facetorch.base.BaseProcessor.optimize">optimize</a></code></li>
<li><code><a title="facetorch.base.BaseReader.process_tensor" href="../../base.html#facetorch.base.BaseReader.process_tensor">process_tensor</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="facetorch.analyzer.reader.core.TensorReader"><code class="flex name class">
<span>class <span class="ident">TensorReader</span></span>
<span>(</span><span>transform:Â torchvision.transforms.transforms.Compose, device:Â torch.device, optimize_transform:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>TensorReader is a wrapper around a functionality for reading tensors by Torchvision.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>transform</code></strong> :&ensp;<code>torchvision.transforms.Compose</code></dt>
<dd>Transform compose object to be applied to the image, if fix_image_size is True.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>Torch device cpu or cuda object.</dd>
<dt><strong><code>optimize_transform</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to optimize the transforms that are: resizing the image to a fixed size.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorReader(BaseReader):
    def __init__(
        self,
        transform: torchvision.transforms.Compose,
        device: torch.device,
        optimize_transform: bool,
    ):
        &#34;&#34;&#34;TensorReader is a wrapper around a functionality for reading tensors by Torchvision.

        Args:
            transform (torchvision.transforms.Compose): Transform compose object to be applied to the image, if fix_image_size is True.
            device (torch.device): Torch device cpu or cuda object.
            optimize_transform (bool): Whether to optimize the transforms that are: resizing the image to a fixed size.

        &#34;&#34;&#34;
        super().__init__(
            transform,
            device,
            optimize_transform,
        )

    @Timer(&#34;TensorReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
    def run(self, tensor: torch.Tensor, fix_img_size: bool = False) -&gt; ImageData:
        &#34;&#34;&#34;Reads a tensor and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

        Args:
            tensor (torch.Tensor): Tensor of a single image with RGB values between 0-255 and shape (channels, height, width).
            fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

        Returns:
            ImageData: ImageData object with image tensor and pil Image.
        &#34;&#34;&#34;
        return self.process_tensor(tensor, fix_img_size)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="facetorch.base.BaseReader" href="../../base.html#facetorch.base.BaseReader">BaseReader</a></li>
<li><a title="facetorch.base.BaseProcessor" href="../../base.html#facetorch.base.BaseProcessor">BaseProcessor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="facetorch.analyzer.reader.core.TensorReader.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, tensor:Â torch.Tensor, fix_img_size:Â boolÂ =Â False) â€‘>Â <a title="facetorch.datastruct.ImageData" href="../../datastruct.html#facetorch.datastruct.ImageData">ImageData</a></span>
</code></dt>
<dd>
<div class="desc"><p>Reads a tensor and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Tensor of a single image with RGB values between 0-255 and shape (channels, height, width).</dd>
<dt><strong><code>fix_img_size</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ImageData</code></dt>
<dd>ImageData object with image tensor and pil Image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Timer(&#34;TensorReader.run&#34;, &#34;{name}: {milliseconds:.2f} ms&#34;, logger=logger.debug)
def run(self, tensor: torch.Tensor, fix_img_size: bool = False) -&gt; ImageData:
    &#34;&#34;&#34;Reads a tensor and returns a tensor of the image with values between 0-255 and shape (batch, channels, height, width). The order of color channels is RGB. PyTorch and Torchvision are used to read the image.

    Args:
        tensor (torch.Tensor): Tensor of a single image with RGB values between 0-255 and shape (channels, height, width).
        fix_img_size (bool): Whether to resize the image to a fixed size. If False, the size_portrait and size_landscape are ignored. Default is False.

    Returns:
        ImageData: ImageData object with image tensor and pil Image.
    &#34;&#34;&#34;
    return self.process_tensor(tensor, fix_img_size)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="facetorch.base.BaseReader" href="../../base.html#facetorch.base.BaseReader">BaseReader</a></b></code>:
<ul class="hlist">
<li><code><a title="facetorch.base.BaseReader.optimize" href="../../base.html#facetorch.base.BaseProcessor.optimize">optimize</a></code></li>
<li><code><a title="facetorch.base.BaseReader.process_tensor" href="../../base.html#facetorch.base.BaseReader.process_tensor">process_tensor</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="ðŸ”Ž Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="facetorch.analyzer.reader" href="index.html">facetorch.analyzer.reader</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="facetorch.analyzer.reader.core.UniversalReader" href="#facetorch.analyzer.reader.core.UniversalReader">UniversalReader</a></code></h4>
<ul class="">
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.run" href="#facetorch.analyzer.reader.core.UniversalReader.run">run</a></code></li>
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.read_tensor" href="#facetorch.analyzer.reader.core.UniversalReader.read_tensor">read_tensor</a></code></li>
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.read_pil_image" href="#facetorch.analyzer.reader.core.UniversalReader.read_pil_image">read_pil_image</a></code></li>
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.read_numpy_array" href="#facetorch.analyzer.reader.core.UniversalReader.read_numpy_array">read_numpy_array</a></code></li>
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.read_image_from_bytes" href="#facetorch.analyzer.reader.core.UniversalReader.read_image_from_bytes">read_image_from_bytes</a></code></li>
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.read_image_from_path" href="#facetorch.analyzer.reader.core.UniversalReader.read_image_from_path">read_image_from_path</a></code></li>
<li><code><a title="facetorch.analyzer.reader.core.UniversalReader.read_image_from_url" href="#facetorch.analyzer.reader.core.UniversalReader.read_image_from_url">read_image_from_url</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="facetorch.analyzer.reader.core.ImageReader" href="#facetorch.analyzer.reader.core.ImageReader">ImageReader</a></code></h4>
<ul class="">
<li><code><a title="facetorch.analyzer.reader.core.ImageReader.run" href="#facetorch.analyzer.reader.core.ImageReader.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="facetorch.analyzer.reader.core.TensorReader" href="#facetorch.analyzer.reader.core.TensorReader">TensorReader</a></code></h4>
<ul class="">
<li><code><a title="facetorch.analyzer.reader.core.TensorReader.run" href="#facetorch.analyzer.reader.core.TensorReader.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>